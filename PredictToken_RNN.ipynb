{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0fHFDe8pyszV"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = pd.read_csv(\"/Game_of_Thrones_Script.csv\")\n",
        "\n",
        "data = data.drop(columns=['Release Date', 'Season', 'Episode', 'Episode Title', 'Name'])\n",
        "sentences = data['Sentence']\n",
        "\n",
        "sentences = sentences.dropna()\n",
        "\n",
        "sentences = sentences.head(5000)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "tPj499qPzOqo",
        "outputId": "937338b1-6d12-477d-8079-4306a4a65461"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_8 (Embedding)     (None, 51, 10)            289960    \n",
            "                                                                 \n",
            " simple_rnn_6 (SimpleRNN)    (None, 51, 128)           17792     \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 51, 28996)         3740484   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4048236 (15.44 MB)\n",
            "Trainable params: 4048236 (15.44 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "500/500 [==============================] - 308s 614ms/step - loss: 3.1171 - accuracy: 0.6720 - val_loss: 2.6683 - val_accuracy: 0.6700\n",
            "Epoch 2/20\n",
            "500/500 [==============================] - 286s 571ms/step - loss: 2.1891 - accuracy: 0.6908 - val_loss: 2.1030 - val_accuracy: 0.6994\n",
            "Epoch 3/20\n",
            "500/500 [==============================] - 274s 548ms/step - loss: 1.9338 - accuracy: 0.7173 - val_loss: 1.8554 - val_accuracy: 0.7235\n",
            "Epoch 4/20\n",
            "500/500 [==============================] - 286s 572ms/step - loss: 1.7377 - accuracy: 0.7318 - val_loss: 1.7651 - val_accuracy: 0.7313\n",
            "Epoch 5/20\n",
            "500/500 [==============================] - 293s 587ms/step - loss: 1.6556 - accuracy: 0.7379 - val_loss: 1.7142 - val_accuracy: 0.7362\n",
            "Epoch 6/20\n",
            "500/500 [==============================] - 281s 562ms/step - loss: 1.5905 - accuracy: 0.7436 - val_loss: 1.6653 - val_accuracy: 0.7427\n",
            "Epoch 7/20\n",
            "500/500 [==============================] - 273s 545ms/step - loss: 1.5368 - accuracy: 0.7478 - val_loss: 1.6417 - val_accuracy: 0.7443\n",
            "Epoch 8/20\n",
            "500/500 [==============================] - 282s 564ms/step - loss: 1.4908 - accuracy: 0.7503 - val_loss: 1.6204 - val_accuracy: 0.7453\n",
            "Epoch 9/20\n",
            "500/500 [==============================] - 270s 541ms/step - loss: 1.4504 - accuracy: 0.7524 - val_loss: 1.6038 - val_accuracy: 0.7469\n",
            "Epoch 10/20\n",
            "500/500 [==============================] - 295s 591ms/step - loss: 1.4113 - accuracy: 0.7549 - val_loss: 1.5897 - val_accuracy: 0.7507\n",
            "Epoch 11/20\n",
            "500/500 [==============================] - 273s 546ms/step - loss: 1.3737 - accuracy: 0.7573 - val_loss: 1.5812 - val_accuracy: 0.7524\n",
            "Epoch 12/20\n",
            "500/500 [==============================] - 271s 542ms/step - loss: 1.3387 - accuracy: 0.7595 - val_loss: 1.5791 - val_accuracy: 0.7530\n",
            "Epoch 13/20\n",
            "370/500 [=====================>........] - ETA: 1:09 - loss: 1.3200 - accuracy: 0.7578"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from transformers import BertTokenizer\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense, LSTM\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "from google.colab import files\n",
        "import io\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "input_sequences = [tokenizer.encode(s, add_special_tokens=True) for s in sentences]\n",
        "\n",
        "sentence_lengths = [len(tokenizer.encode(s, add_special_tokens=True)) for s in sentences]\n",
        "max_sequence_len = 52\n",
        "\n",
        "vocab_size = len(tokenizer.vocab)\n",
        "\n",
        "x = [seq[:-1] for seq in input_sequences]\n",
        "y = [seq[1:] for seq in input_sequences]\n",
        "\n",
        "x = pad_sequences(x, maxlen= max_sequence_len-1, padding='pre')\n",
        "y = pad_sequences(y, maxlen= max_sequence_len-1, padding='pre')\n",
        "\n",
        "x_train, x_temp, y_train, y_temp = train_test_split(x, y, test_size=0.2, random_state=123)\n",
        "x_val, x_test, y_val, y_test = train_test_split(x_temp, y_temp, test_size=0.5, random_state=123)\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 10, input_length= max_sequence_len-1))\n",
        "model.add(SimpleRNN(128, return_sequences=True))\n",
        "model.add(Dense(vocab_size, activation= 'softmax'))\n",
        "model.compile(loss='sparse_categorical_crossentropy',\n",
        "              optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "epochs = 20\n",
        "batch_size = 8\n",
        "\n",
        "history = model.fit(x_train,\n",
        "                          np.expand_dims(y_train, -1),\n",
        "                          validation_data=(x_val, np.expand_dims(y_val, -1)),\n",
        "                          epochs= epochs, batch_size= batch_size)\n",
        "\n",
        "loss, accuracy = transformer.evaluate(x_test, np.expand_dims(y_test, -1))\n",
        "print('Test Loss:', loss)\n",
        "print('Test Accuracy:', accuracy)\n",
        "\n",
        "plt.figure(figsize=(18, 4))\n",
        "\n",
        "# Plot training & validation accuracy values\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kNThUX7wMFmX"
      },
      "outputs": [],
      "source": [
        "\n",
        "plt.figure(figsize=(18, 4))\n",
        "\n",
        "# Plot Accuracy\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "# Plot Loss\n",
        "plt.subplot(1, 2, 2)  # Use the second subplot\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"RNN Accuracy and Loss Graph.png\")\n",
        "plt.show()\n",
        "files.download(\"RNN Accuracy and Loss Graph.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GfL1L8eAQZrL"
      },
      "outputs": [],
      "source": [
        "model.save(\"rnn_model.h5\")\n",
        "files.download(\"rnn_model.h5\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
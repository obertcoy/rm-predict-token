{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RQ_uRQtRQkYd"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = pd.read_csv(\"/Game_of_Thrones_Script.csv\")\n",
        "\n",
        "data = data.drop(columns=['Release Date', 'Season', 'Episode', 'Episode Title', 'Name'])\n",
        "sentences = data['Sentence']\n",
        "\n",
        "sentences = sentences.dropna()\n",
        "\n",
        "sentences = sentences.head(5000)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from transformers import BertTokenizer\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense, LSTM\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "from google.colab import files\n",
        "import io\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "input_sequences = [tokenizer.encode(s, add_special_tokens=True) for s in sentences]\n",
        "\n",
        "sentence_lengths = [len(tokenizer.encode(s, add_special_tokens=True)) for s in sentences]\n",
        "max_sequence_len = 52\n",
        "\n",
        "vocab_size = len(tokenizer.vocab)\n",
        "\n",
        "x = [seq[:-1] for seq in input_sequences]\n",
        "y = [seq[1:] for seq in input_sequences]\n",
        "\n",
        "x = pad_sequences(x, maxlen= max_sequence_len-1, padding='pre')\n",
        "y = pad_sequences(y, maxlen= max_sequence_len-1, padding='pre')\n",
        "\n",
        "x_train, x_temp, y_train, y_temp = train_test_split(x, y, test_size=0.2, random_state=123)\n",
        "x_val, x_test, y_val, y_test = train_test_split(x_temp, y_temp, test_size=0.5, random_state=123)\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 10, input_length= max_sequence_len-1))\n",
        "model.add(LSTM(128, return_sequences=True))\n",
        "model.add(Dense(vocab_size, activation= 'softmax'))\n",
        "model.compile(loss='sparse_categorical_crossentropy',\n",
        "              optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "epochs = 20\n",
        "batch_size = 8\n",
        "\n",
        "history = model.fit(x_train,\n",
        "                          np.expand_dims(y_train, -1),\n",
        "                          validation_data=(x_val, np.expand_dims(y_val, -1)),\n",
        "                          epochs= epochs, batch_size= batch_size)\n",
        "\n",
        "loss, accuracy = transformer.evaluate(x_test, np.expand_dims(y_test, -1))\n",
        "print('Test Loss:', loss)\n",
        "print('Test Accuracy:', accuracy)\n",
        "\n",
        "plt.figure(figsize=(18, 4))\n",
        "\n",
        "# Plot training & validation accuracy values\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BRocY2fMQmzB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "plt.figure(figsize=(18, 4))\n",
        "\n",
        "# Plot Accuracy\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "# Plot Loss\n",
        "plt.subplot(1, 2, 2)  # Use the second subplot\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"LSTM Accuracy and Loss Graph.png\")\n",
        "plt.show()\n",
        "files.download(\"LSTM Accuracy and Loss Graph.png\")"
      ],
      "metadata": {
        "id": "9hb89HAOQrdf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"lstm_model.h5\")\n",
        "files.download(\"lstm_model.h5\")"
      ],
      "metadata": {
        "id": "BlO8ftazQxfw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}